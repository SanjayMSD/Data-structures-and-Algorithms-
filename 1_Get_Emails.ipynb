{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOjA2ookBXwbpGCzfRVqyj5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SanjayMSD/Data-structures-and-Algorithms-/blob/master/1_Get_Emails.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "c_VjBvSdo9bz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0b1BN22Noqaa"
      },
      "outputs": [],
      "source": [
        "# --- Install dependencies (only once per runtime)\n",
        "!pip install pandas requests openpyxl beautifulsoup4 lxml"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/Personal Colab/\")"
      ],
      "metadata": {
        "id": "PKgoxeFdo7x0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import time\n",
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "from datetime import timedelta\n",
        "\n",
        "# --- SETTINGS ---\n",
        "INPUT_FILE = \"filtered_companies.csv\"     # Input CSV file\n",
        "OUTPUT_EXCEL = \"combined_with_emails.xlsx\"  # Output Excel file\n",
        "TIMEOUT = 30\n",
        "MAX_WEBSITES = 900000       # optional limit\n",
        "RETRIES = 3\n",
        "\n",
        "# --- HTTP session setup ---\n",
        "session = requests.Session()\n",
        "session.headers.update({\"User-Agent\": \"Mozilla/5.0\"})\n",
        "\n",
        "def extract_emails_from_text(text):\n",
        "    \"\"\"Extract email addresses from text.\"\"\"\n",
        "    pattern = r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\"\n",
        "    return list(set(re.findall(pattern, text)))\n",
        "\n",
        "def fetch_page_emails(url, retries=RETRIES):\n",
        "    \"\"\"Fetch emails from a single page.\"\"\"\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            r = session.get(url, timeout=TIMEOUT)\n",
        "            if r.status_code == 200:\n",
        "                soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "                return extract_emails_from_text(soup.get_text(\" \"))\n",
        "        except Exception as e:\n",
        "            if attempt < retries - 1:\n",
        "                time.sleep(2)\n",
        "            else:\n",
        "                print(f\"[WARN] Failed to fetch {url}: {e}\")\n",
        "    return []\n",
        "\n",
        "def crawl_website(base_url):\n",
        "    \"\"\"Fetch homepage + About/Contact pages.\"\"\"\n",
        "    emails = set()\n",
        "    emails.update(fetch_page_emails(base_url))\n",
        "    try:\n",
        "        r = session.get(base_url, timeout=TIMEOUT)\n",
        "        if r.status_code == 200:\n",
        "            soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "            for link in soup.find_all(\"a\", href=True):\n",
        "                href = link[\"href\"].lower()\n",
        "                if any(x in href for x in [\"about\", \"contact\"]):\n",
        "                    sub_url = urljoin(base_url, href)\n",
        "                    emails.update(fetch_page_emails(sub_url))\n",
        "    except Exception:\n",
        "        pass\n",
        "    return list({e.lower() for e in emails})\n",
        "\n",
        "def format_time(sec):\n",
        "    return str(timedelta(seconds=int(sec)))\n",
        "\n",
        "# --- MAIN SCRIPT ---\n",
        "if not os.path.exists(INPUT_FILE):\n",
        "    raise FileNotFoundError(f\"❌ File '{INPUT_FILE}' not found!\")\n",
        "\n",
        "df = pd.read_csv(INPUT_FILE)\n",
        "\n",
        "# Ensure required columns exist\n",
        "if not any(c.lower() == \"website\" for c in df.columns):\n",
        "    raise ValueError(\"❌ No 'Website' column found in your CSV!\")\n",
        "\n",
        "if \"Got Email\" not in df.columns:\n",
        "    df[\"Got Email\"] = \"No\"\n",
        "\n",
        "# Normalize column names\n",
        "df.columns = [c.strip() for c in df.columns]\n",
        "website_col = next(c for c in df.columns if c.lower() == \"website\")\n",
        "\n",
        "# --- Filter only rows with Got Email = \"No\" ---\n",
        "df_pending = df[df[\"Got Email\"].astype(str).str.strip().str.lower() == \"no\"]\n",
        "df_pending = df_pending.dropna(subset=[website_col])\n",
        "df_pending[website_col] = df_pending[website_col].astype(str).str.strip()\n",
        "df_pending = df_pending[df_pending[website_col] != \"\"].drop_duplicates(subset=[website_col])\n",
        "\n",
        "# Optional: limit number of sites per run\n",
        "df_pending = df_pending.head(MAX_WEBSITES)\n",
        "total = len(df_pending)\n",
        "print(f\"\\n📊 Websites to process this run: {total}\\n\")\n",
        "\n",
        "results = []\n",
        "total_time = 0\n",
        "\n",
        "for i, (idx, row) in enumerate(df_pending.iterrows(), start=1):\n",
        "    url = row[website_col]\n",
        "    if not url.startswith(\"http\"):\n",
        "        url = \"http://\" + url\n",
        "\n",
        "    print(f\"[INFO] ({i}/{total}) Crawling: {url}\")\n",
        "    start = time.time()\n",
        "\n",
        "    emails = crawl_website(url)\n",
        "    got_email = \"Yes\" if emails else \"No\"\n",
        "\n",
        "    # Update DataFrame\n",
        "    df.loc[idx, \"Got Email\"] = got_email\n",
        "\n",
        "    if not emails:\n",
        "        emails = [\"No emails found\"]\n",
        "\n",
        "    for e in emails:\n",
        "        results.append({\"Website\": url, \"Email\": e, \"Got Email\": got_email})\n",
        "\n",
        "    elapsed = time.time() - start\n",
        "    total_time += elapsed\n",
        "    avg = total_time / i\n",
        "    eta = avg * (total - i)\n",
        "    print(f\"✅ {url} → {', '.join(emails)} | Marked: {got_email} | ⏱ ETA: {format_time(eta)}\")\n",
        "\n",
        "# --- Save results ---\n",
        "# Save found emails to Excel\n",
        "result_df = pd.DataFrame(results)\n",
        "result_df.to_excel(OUTPUT_EXCEL, index=False)\n",
        "\n",
        "# Save updated CSV (for next run continuation)\n",
        "df.to_csv(INPUT_FILE, index=False)\n",
        "\n",
        "print(f\"\\n✅ [DONE] Results saved to '{OUTPUT_EXCEL}'\")\n",
        "print(f\"🔁 Updated '{INPUT_FILE}' — Next run will continue only for rows with 'Got Email' = 'No'\")\n"
      ],
      "metadata": {
        "id": "DumQqUjopIsj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}